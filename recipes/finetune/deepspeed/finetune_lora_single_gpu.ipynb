{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e6981ab-2d9a-4280-923f-235a166855ba",
   "metadata": {},
   "source": [
    "# LoRA Fine-Tuning Qwen-Chat Large Language Model (Single GPU)\n",
    "\n",
    "Tongyi Qianwen is a large language model developed by Alibaba Cloud based on the Transformer architecture, trained on an extensive set of pre-training data. The pre-training data is diverse and covers a wide range, including a large amount of internet text, specialized books, code, etc. In addition, an AI assistant called Qwen-Chat has been created based on the pre-trained model using alignment mechanism.\n",
    "\n",
    "This notebook uses Qwen-1.8B-Chat as an example to introduce how to LoRA fine-tune the Qianwen model using Deepspeed.\n",
    "\n",
    "## Environment Requirements\n",
    "\n",
    "Please refer to **requirements.txt** to install the required dependencies.\n",
    "\n",
    "## Preparation\n",
    "\n",
    "### Download Qwen-1.8B-Chat\n",
    "\n",
    "First, download the model files. You can choose to download directly from ModelScope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f228a718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zjy/llm_journey/Qwen/recipes/finetune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zjy/llm_journey/Qwen/qwen/lib/python3.12/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/zjy/llm_journey/Qwen/recipes/finetune'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %pwd\n",
    "# %cd ../\n",
    "# %pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "248488f9-4a86-4f35-9d56-50f8e91a8f11",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model to directory: ./hub/Qwen/Qwen-1_8B-Chat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 10:27:55,084 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n",
      "Downloading [cache_autogptq_cuda_kernel_256.cu]: 100%|██████████| 50.8k/50.8k [00:01<00:00, 30.7kB/s]\n",
      "Downloading [config.json]: 100%|██████████| 910/910 [00:01<00:00, 484B/s]\n",
      "Downloading [configuration.json]: 100%|██████████| 77.0/77.0 [00:02<00:00, 31.4B/s]\n",
      "Downloading [configuration_qwen.py]: 100%|██████████| 2.29k/2.29k [00:01<00:00, 1.20kB/s]\n",
      "Downloading [cpp_kernels.py]: 100%|██████████| 1.88k/1.88k [00:01<00:00, 1.18kB/s]\n",
      "Downloading [generation_config.json]: 100%|██████████| 249/249 [00:01<00:00, 143B/s]\n",
      "Downloading [LICENSE]: 100%|██████████| 7.11k/7.11k [00:01<00:00, 5.06kB/s]\n",
      "Downloading [assets/logo.jpg]: 100%|██████████| 80.8k/80.8k [00:01<00:00, 46.0kB/s]\n",
      "Downloading [model-00001-of-00002.safetensors]: 100%|██████████| 1.90G/1.90G [01:04<00:00, 31.9MB/s]\n",
      "Downloading [model-00002-of-00002.safetensors]: 100%|██████████| 1.52G/1.52G [00:57<00:00, 28.4MB/s]\n",
      "Downloading [model.safetensors.index.json]: 100%|██████████| 14.4k/14.4k [00:01<00:00, 9.75kB/s]\n",
      "Downloading [modeling_qwen.py]: 100%|██████████| 54.3k/54.3k [00:01<00:00, 30.9kB/s]\n",
      "Downloading [NOTICE]: 100%|██████████| 15.0k/15.0k [00:01<00:00, 7.85kB/s]\n",
      "Downloading [qwen.tiktoken]: 100%|██████████| 2.44M/2.44M [00:02<00:00, 886kB/s]\n",
      "\n",
      "Downloading [qwen_generation_utils.py]:   0%|          | 0.00/14.3k [01:00<?, ?B/s]\n",
      "\n",
      "Downloading [qwen_generation_utils.py]: 100%|██████████| 14.3k/14.3k [00:03<00:00, 4.70kB/s]\n",
      "Downloading [assets/qwen_tokenizer.png]: 100%|██████████| 79.0k/79.0k [00:02<00:00, 38.3kB/s]\n",
      "Downloading [examples/react_prompt.md]: 100%|██████████| 11.6k/11.6k [00:01<00:00, 7.84kB/s]\n",
      "Downloading [assets/react_showcase_001.png]: 100%|██████████| 302k/302k [00:02<00:00, 137kB/s]\n",
      "Downloading [assets/react_showcase_002.png]: 100%|██████████| 615k/615k [00:02<00:00, 236kB/s]\n",
      "Downloading [README.md]: 100%|██████████| 25.9k/25.9k [00:02<00:00, 12.0kB/s]\n",
      "Downloading [tokenization_qwen.py]: 100%|██████████| 9.39k/9.39k [00:01<00:00, 6.70kB/s]\n",
      "Downloading [tokenizer_config.json]: 100%|██████████| 173/173 [00:01<00:00, 129B/s]\n",
      "Downloading [assets/wechat.png]: 100%|██████████| 66.8k/66.8k [00:01<00:00, 38.4kB/s]\n"
     ]
    }
   ],
   "source": [
    "from modelscope.hub.snapshot_download import snapshot_download\n",
    "model_dir = snapshot_download('Qwen/Qwen-1_8B-Chat', cache_dir='.', revision='master')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b2a92b1-f08e-4413-9f92-8f23761e6e1f",
   "metadata": {},
   "source": [
    "### Download Example Training Data\n",
    "\n",
    "Download the data required for training; here, we provide a tiny dataset as an example. It is sampled from [Belle](https://github.com/LianjiaTech/BELLE).\n",
    "\n",
    "Disclaimer: the dataset can be only used for the research purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce195f08-fbb2-470e-b6c0-9a03457458c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-15 10:32:13--  https://atp-modelzoo-sh.oss-cn-shanghai.aliyuncs.com/release/tutorials/qwen_recipes/Belle_sampled_qwen.json\n",
      "Resolving atp-modelzoo-sh.oss-cn-shanghai.aliyuncs.com (atp-modelzoo-sh.oss-cn-shanghai.aliyuncs.com)... 47.101.88.43\n",
      "Connecting to atp-modelzoo-sh.oss-cn-shanghai.aliyuncs.com (atp-modelzoo-sh.oss-cn-shanghai.aliyuncs.com)|47.101.88.43|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 228189 (223K) [application/json]\n",
      "Saving to: ‘Belle_sampled_qwen.json’\n",
      "\n",
      "Belle_sampled_qwen. 100%[===================>] 222.84K   530KB/s    in 0.4s    \n",
      "\n",
      "2024-11-15 10:32:15 (530 KB/s) - ‘Belle_sampled_qwen.json’ saved [228189/228189]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://atp-modelzoo-sh.oss-cn-shanghai.aliyuncs.com/release/tutorials/qwen_recipes/Belle_sampled_qwen.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7226bed0-171b-4d45-a3f9-b3d81ec2bb9f",
   "metadata": {},
   "source": [
    "You can also refer to this format to prepare the dataset. Below is a simple example list with 1 sample:\n",
    "\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"id\": \"identity_0\",\n",
    "    \"conversations\": [\n",
    "      {\n",
    "        \"from\": \"user\",\n",
    "        \"value\": \"你好\"\n",
    "      },\n",
    "      {\n",
    "        \"from\": \"assistant\",\n",
    "        \"value\": \"我是一个语言模型，我叫通义千问。\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "You can also use multi-turn conversations as the training set. Here is a simple example:\n",
    "\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"id\": \"identity_0\",\n",
    "    \"conversations\": [\n",
    "      {\n",
    "        \"from\": \"user\",\n",
    "        \"value\": \"你好，能告诉我遛狗的最佳时间吗？\"\n",
    "      },\n",
    "      {\n",
    "        \"from\": \"assistant\",\n",
    "        \"value\": \"当地最佳遛狗时间因地域差异而异，请问您所在的城市是哪里？\"\n",
    "      },\n",
    "      {\n",
    "        \"from\": \"user\",\n",
    "        \"value\": \"我在纽约市。\"\n",
    "      },\n",
    "      {\n",
    "        \"from\": \"assistant\",\n",
    "        \"value\": \"纽约市的遛狗最佳时间通常在早晨6点至8点和晚上8点至10点之间，因为这些时间段气温较低，遛狗更加舒适。但具体时间还需根据气候、气温和季节变化而定。\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "## Fine-Tune the Model\n",
    "\n",
    "You can directly run the prepared training script to fine-tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ab0581e-be85-45e6-a5b7-af9c42ea697b",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-15 10:33:12,618] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to mps (auto detect)\n",
      "W1115 10:33:13.226000 34929 qwen/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "/Users/zjy/llm_journey/Qwen/qwen/lib/python3.12/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:06<00:00,  3.06s/it]\n",
      "trainable params: 53,673,984 || all params: 1,890,502,656 || trainable%: 2.8391\n",
      "Loading data...\n",
      "Formatting inputs...Skip in lazy mode\n",
      "/Users/zjy/llm_journey/Qwen/recipes/finetune/../../finetune.py:364: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n",
      "  0%|                                                    | 0/80 [00:00<?, ?it/s]/Users/zjy/llm_journey/Qwen/qwen/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "{'loss': 1.035, 'grad_norm': 5.1622772216796875, 'learning_rate': 1e-05, 'epoch': 0.06}\n",
      "{'loss': 1.0264, 'grad_norm': 4.078629016876221, 'learning_rate': 9.99604698613651e-06, 'epoch': 0.12}\n",
      "{'loss': 0.7152, 'grad_norm': 5.781802654266357, 'learning_rate': 9.98419419507348e-06, 'epoch': 0.18}\n",
      "{'loss': 0.825, 'grad_norm': 5.717673301696777, 'learning_rate': 9.964460368509868e-06, 'epoch': 0.24}\n",
      "{'loss': 0.9382, 'grad_norm': 5.715182781219482, 'learning_rate': 9.936876709681668e-06, 'epoch': 0.3}\n",
      "{'loss': 0.7334, 'grad_norm': 4.8090643882751465, 'learning_rate': 9.901486834023182e-06, 'epoch': 0.36}\n",
      "{'loss': 0.968, 'grad_norm': 6.176565170288086, 'learning_rate': 9.85834670020205e-06, 'epoch': 0.42}\n",
      "{'loss': 0.9656, 'grad_norm': 4.485891342163086, 'learning_rate': 9.807524521637103e-06, 'epoch': 0.48}\n",
      "{'loss': 0.9317, 'grad_norm': 6.785899639129639, 'learning_rate': 9.749100658638914e-06, 'epoch': 0.54}\n",
      "{'loss': 0.6533, 'grad_norm': 4.353843688964844, 'learning_rate': 9.68316749134364e-06, 'epoch': 0.6}\n",
      "{'loss': 0.9021, 'grad_norm': 5.597474098205566, 'learning_rate': 9.609829273641034e-06, 'epoch': 0.66}\n",
      "{'loss': 0.6869, 'grad_norm': 6.517662525177002, 'learning_rate': 9.529201968327618e-06, 'epoch': 0.72}\n",
      "{'loss': 0.8264, 'grad_norm': 4.081151962280273, 'learning_rate': 9.44141306374566e-06, 'epoch': 0.78}\n",
      "{'loss': 0.8148, 'grad_norm': 5.576452732086182, 'learning_rate': 9.346601372197914e-06, 'epoch': 0.84}\n",
      "{'loss': 0.8326, 'grad_norm': 6.580228328704834, 'learning_rate': 9.244916810456822e-06, 'epoch': 0.9}\n",
      "{'loss': 0.8467, 'grad_norm': 4.5379838943481445, 'learning_rate': 9.136520162715288e-06, 'epoch': 0.96}\n",
      "{'loss': 0.9218, 'grad_norm': 6.246469974517822, 'learning_rate': 9.021582826353825e-06, 'epoch': 1.02}\n",
      "{'loss': 0.8494, 'grad_norm': 4.972136974334717, 'learning_rate': 8.900286540926062e-06, 'epoch': 1.08}\n",
      "{'loss': 0.7554, 'grad_norm': 4.3841986656188965, 'learning_rate': 8.772823100791152e-06, 'epoch': 1.14}\n",
      "{'loss': 0.8978, 'grad_norm': 4.059811115264893, 'learning_rate': 8.639394051847472e-06, 'epoch': 1.2}\n",
      "{'loss': 0.8493, 'grad_norm': 5.9042840003967285, 'learning_rate': 8.500210372847128e-06, 'epoch': 1.26}\n",
      "{'loss': 0.7753, 'grad_norm': 4.504127502441406, 'learning_rate': 8.355492141795185e-06, 'epoch': 1.32}\n",
      "{'loss': 0.7823, 'grad_norm': 4.070717811584473, 'learning_rate': 8.2054681879611e-06, 'epoch': 1.38}\n",
      "{'loss': 0.9039, 'grad_norm': 4.480990886688232, 'learning_rate': 8.050375730052623e-06, 'epoch': 1.44}\n",
      "{'loss': 0.8019, 'grad_norm': 4.179333209991455, 'learning_rate': 7.890460001124242e-06, 'epoch': 1.5}\n",
      "{'loss': 0.964, 'grad_norm': 4.743725299835205, 'learning_rate': 7.725973860813338e-06, 'epoch': 1.56}\n",
      "{'loss': 0.9067, 'grad_norm': 5.047615051269531, 'learning_rate': 7.5571773955171124e-06, 'epoch': 1.62}\n",
      "{'loss': 0.8181, 'grad_norm': 5.650163173675537, 'learning_rate': 7.3843375071425315e-06, 'epoch': 1.68}\n",
      "{'loss': 0.9456, 'grad_norm': 5.578127384185791, 'learning_rate': 7.2077274910795605e-06, 'epoch': 1.74}\n",
      "{'loss': 0.6889, 'grad_norm': 3.474552631378174, 'learning_rate': 7.02762660406497e-06, 'epoch': 1.8}\n",
      "{'loss': 0.6942, 'grad_norm': 4.596667766571045, 'learning_rate': 6.844319622620039e-06, 'epoch': 1.86}\n",
      "{'loss': 0.4909, 'grad_norm': 3.7791898250579834, 'learning_rate': 6.65809639276034e-06, 'epoch': 1.92}\n",
      "{'loss': 0.734, 'grad_norm': 3.2934184074401855, 'learning_rate': 6.469251371689606e-06, 'epoch': 1.98}\n",
      "{'loss': 0.8492, 'grad_norm': 5.371791362762451, 'learning_rate': 6.278083162202374e-06, 'epoch': 2.04}\n",
      "{'loss': 0.7505, 'grad_norm': 4.758277893066406, 'learning_rate': 6.084894040531591e-06, 'epoch': 2.1}\n",
      "{'loss': 0.7726, 'grad_norm': 4.501697063446045, 'learning_rate': 5.8899894783877536e-06, 'epoch': 2.16}\n",
      "{'loss': 0.6748, 'grad_norm': 4.079547882080078, 'learning_rate': 5.693677659945343e-06, 'epoch': 2.22}\n",
      "{'loss': 0.6691, 'grad_norm': 4.856786727905273, 'learning_rate': 5.496268994540309e-06, 'epoch': 2.28}\n",
      "{'loss': 0.6963, 'grad_norm': 3.4810547828674316, 'learning_rate': 5.2980756258491e-06, 'epoch': 2.34}\n",
      "{'loss': 0.8492, 'grad_norm': 4.2446441650390625, 'learning_rate': 5.099410938325351e-06, 'epoch': 2.4}\n",
      "{'loss': 0.882, 'grad_norm': 4.098357200622559, 'learning_rate': 4.900589061674649e-06, 'epoch': 2.46}\n",
      "{'loss': 0.8962, 'grad_norm': 3.8160812854766846, 'learning_rate': 4.701924374150901e-06, 'epoch': 2.52}\n",
      "{'loss': 0.6679, 'grad_norm': 4.80321741104126, 'learning_rate': 4.5037310054596936e-06, 'epoch': 2.58}\n",
      "{'loss': 0.905, 'grad_norm': 4.543826103210449, 'learning_rate': 4.30632234005466e-06, 'epoch': 2.64}\n",
      "{'loss': 0.6998, 'grad_norm': 4.6856255531311035, 'learning_rate': 4.11001052161225e-06, 'epoch': 2.7}\n",
      "{'loss': 0.6735, 'grad_norm': 3.9663684368133545, 'learning_rate': 3.91510595946841e-06, 'epoch': 2.76}\n",
      "{'loss': 0.7922, 'grad_norm': 4.065088272094727, 'learning_rate': 3.721916837797627e-06, 'epoch': 2.82}\n",
      "{'loss': 0.8319, 'grad_norm': 4.772068023681641, 'learning_rate': 3.5307486283103966e-06, 'epoch': 2.88}\n",
      "{'loss': 0.8971, 'grad_norm': 4.337107181549072, 'learning_rate': 3.3419036072396614e-06, 'epoch': 2.94}\n",
      "{'loss': 0.8454, 'grad_norm': 4.671784400939941, 'learning_rate': 3.1556803773799616e-06, 'epoch': 3.0}\n",
      "{'loss': 0.754, 'grad_norm': 4.853881359100342, 'learning_rate': 2.972373395935031e-06, 'epoch': 3.06}\n",
      "{'loss': 0.8493, 'grad_norm': 3.6191303730010986, 'learning_rate': 2.792272508920443e-06, 'epoch': 3.12}\n",
      "{'loss': 0.6945, 'grad_norm': 4.216838359832764, 'learning_rate': 2.615662492857471e-06, 'epoch': 3.18}\n",
      "{'loss': 0.6956, 'grad_norm': 4.112166881561279, 'learning_rate': 2.4428226044828896e-06, 'epoch': 3.24}\n",
      "{'loss': 0.8322, 'grad_norm': 4.0934600830078125, 'learning_rate': 2.2740261391866634e-06, 'epoch': 3.3}\n",
      "{'loss': 0.7941, 'grad_norm': 4.693774223327637, 'learning_rate': 2.109539998875757e-06, 'epoch': 3.36}\n",
      "{'loss': 0.838, 'grad_norm': 4.202505111694336, 'learning_rate': 1.949624269947378e-06, 'epoch': 3.42}\n",
      "{'loss': 0.5498, 'grad_norm': 3.8740243911743164, 'learning_rate': 1.794531812038901e-06, 'epoch': 3.48}\n",
      "{'loss': 0.6835, 'grad_norm': 4.039596080780029, 'learning_rate': 1.6445078582048158e-06, 'epoch': 3.54}\n",
      "{'loss': 0.7872, 'grad_norm': 3.8412694931030273, 'learning_rate': 1.499789627152874e-06, 'epoch': 3.6}\n",
      "{'loss': 0.7616, 'grad_norm': 4.56403112411499, 'learning_rate': 1.3606059481525296e-06, 'epoch': 3.66}\n",
      "{'loss': 0.7432, 'grad_norm': 4.36243200302124, 'learning_rate': 1.227176899208849e-06, 'epoch': 3.72}\n",
      "{'loss': 0.8363, 'grad_norm': 3.5047082901000977, 'learning_rate': 1.09971345907394e-06, 'epoch': 3.78}\n",
      "{'loss': 0.731, 'grad_norm': 4.156765460968018, 'learning_rate': 9.784171736461762e-07, 'epoch': 3.84}\n",
      "{'loss': 0.6459, 'grad_norm': 3.7369439601898193, 'learning_rate': 8.634798372847148e-07, 'epoch': 3.9}\n",
      "{'loss': 0.8721, 'grad_norm': 5.273192882537842, 'learning_rate': 7.550831895431799e-07, 'epoch': 3.96}\n",
      "{'loss': 1.0929, 'grad_norm': 4.706140995025635, 'learning_rate': 6.533986278020876e-07, 'epoch': 4.01}\n",
      "{'loss': 0.8627, 'grad_norm': 4.617386817932129, 'learning_rate': 5.585869362543416e-07, 'epoch': 4.07}\n",
      "{'loss': 0.7376, 'grad_norm': 4.293893814086914, 'learning_rate': 4.707980316723837e-07, 'epoch': 4.13}\n",
      "{'loss': 0.8108, 'grad_norm': 4.994075775146484, 'learning_rate': 3.9017072635896716e-07, 'epoch': 4.19}\n",
      "{'loss': 0.8539, 'grad_norm': 3.4912614822387695, 'learning_rate': 3.168325086563612e-07, 'epoch': 4.25}\n",
      "{'loss': 0.7904, 'grad_norm': 4.384023666381836, 'learning_rate': 2.5089934136108665e-07, 'epoch': 4.31}\n",
      "{'loss': 0.7134, 'grad_norm': 3.9337055683135986, 'learning_rate': 1.9247547836289792e-07, 'epoch': 4.37}\n",
      "{'loss': 0.9315, 'grad_norm': 4.131223201751709, 'learning_rate': 1.4165329979794972e-07, 'epoch': 4.43}\n",
      "{'loss': 0.8102, 'grad_norm': 3.969045400619507, 'learning_rate': 9.851316597681959e-08, 'epoch': 4.49}\n",
      "{'loss': 0.7966, 'grad_norm': 4.422358512878418, 'learning_rate': 6.31232903183332e-08, 'epoch': 4.55}\n",
      "{'loss': 0.6032, 'grad_norm': 4.000452518463135, 'learning_rate': 3.553963149013295e-08, 'epoch': 4.61}\n",
      "{'loss': 0.6571, 'grad_norm': 4.284400939941406, 'learning_rate': 1.580580492652084e-08, 'epoch': 4.67}\n",
      "{'loss': 0.7421, 'grad_norm': 4.000175476074219, 'learning_rate': 3.953013863490784e-09, 'epoch': 4.73}\n",
      "{'loss': 0.8212, 'grad_norm': 4.563438415527344, 'learning_rate': 0.0, 'epoch': 4.79}\n",
      "{'train_runtime': 5887.729, 'train_samples_per_second': 0.227, 'train_steps_per_second': 0.014, 'train_loss': 0.8015662714838981, 'epoch': 4.79}\n",
      "100%|█████████████████████████████████████████| 80/80 [1:38:07<00:00, 73.60s/it]\n"
     ]
    }
   ],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0\n",
    "!python ../../finetune.py \\\n",
    "    --model_name_or_path \"Qwen/Qwen-1_8B-Chat/\"\\\n",
    "    --data_path  \"Belle_sampled_qwen.json\"\\\n",
    "    --bf16 \\\n",
    "    --output_dir \"output_qwen\" \\\n",
    "    --num_train_epochs 5 \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --gradient_accumulation_steps 16 \\\n",
    "    --evaluation_strategy \"no\" \\\n",
    "    --save_strategy \"steps\" \\\n",
    "    --save_steps 1000 \\\n",
    "    --save_total_limit 10 \\\n",
    "    --learning_rate 1e-5 \\\n",
    "    --weight_decay 0.1 \\\n",
    "    --adam_beta2 0.95 \\\n",
    "    --warmup_ratio 0.01 \\\n",
    "    --lr_scheduler_type \"cosine\" \\\n",
    "    --logging_steps 1 \\\n",
    "    --report_to \"none\" \\\n",
    "    --model_max_length 512 \\\n",
    "    --gradient_checkpointing \\\n",
    "    --lazy_preprocess \\\n",
    "    --use_lora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6f28aa-1772-48ce-aa15-8cf29e7d67b5",
   "metadata": {},
   "source": [
    "## Merge Weights\n",
    "\n",
    "The training of both LoRA and Q-LoRA only saves the adapter parameters. You can load the fine-tuned model and merge weights as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fd5ef2a-34f9-4909-bebe-7b3b086fd16a",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-15 12:12:15,849] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to mps (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1115 12:12:16.075000 33825 torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-1_8B-Chat/\", torch_dtype=torch.float16, device_map=\"auto\", trust_remote_code=True)\n",
    "model = PeftModel.from_pretrained(model, \"output_qwen/\")\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"output_qwen_merged\", max_shard_size=\"2048MB\", safe_serialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3f5b9f-63a1-4599-8d9b-a8d8f764838f",
   "metadata": {},
   "source": [
    "The tokenizer files are not saved in the new directory in this step. You can copy the tokenizer files or use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10fa5ea3-dd55-4901-86af-c045d4c56533",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('output_qwen_merged/tokenizer_config.json',\n",
       " 'output_qwen_merged/special_tokens_map.json',\n",
       " 'output_qwen_merged/qwen.tiktoken',\n",
       " 'output_qwen_merged/added_tokens.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Qwen/Qwen-1_8B-Chat/\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer.save_pretrained(\"output_qwen_merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804b84d8",
   "metadata": {},
   "source": [
    "## Test the Model\n",
    "\n",
    "### added the modeling_qwen.py, qwen_generation_utils.py, qwen_generation_utils.py from original Qwen/Qwen-1_8B-Chat\n",
    "\n",
    "After merging the weights, we can test the model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading [cache_autogptq_cuda_kernel_256.cu]:   0%|          | 0.00/50.8k [1:50:45<?, ?B/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.92s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the disk.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！有什么我能帮助你的吗？\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation import GenerationConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"output_qwen_merged\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"output_qwen_merged\",\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ").eval()\n",
    "\n",
    "response, history = model.chat(tokenizer, \"你好\", history=None)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ede03cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！有什么我可以帮助你的吗？\n"
     ]
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, \"你好\", history=None)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "099d4226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国的首都是北京。\n"
     ]
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, \"请问中国的首都是哪个城市？\", history=None)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ee65f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "现在的法定结婚年龄是男满20周岁，女满18周岁。\n"
     ]
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, \"大学生一般多少岁？\", history=None)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c96360b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随着科技的发展，人们的生活方式发生了翻天覆地的变化。手机、人工智能等技术使得生活更为便捷和舒适。然而，这也可能带来一些负面影响，如传统行业和职业的冲击以及生产消费模式的改变。\n"
     ]
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, \"总结下面这段文本的摘要，随着科技飞速发展，我们的生活方式发生巨大改变。手机、人工智能、物联网的出现，让日常生活便利而舒适。比如，我们可以通过智能手机随时随地地获取信息，控制家庭设备，同时感受着人工智能为我们带来的智能化之便。但是，科技进步带来的便利也会对生活形成某种影响，比如，冲击传统行业和职业，改变人们的生产和消费模式。\", history=None)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e20c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "response, history = model.chat(tokenizer, \"总结下面这段文本的摘要，随着科技的飞速发展，我们的生活方式也在悄然改变。智能手机、人工智能、物联网等科技产品的出现，为我们的日常生活带来了更多便利和舒适。比如，我们可以通过智能手机随时随地地获取信息，控制家庭设备，同时感受着人工智能为我们带来的智能化之便。但是，科技进步带来的便利也会对生活形成某种影响，比如，冲击传统行业和职业，改变人们的生产和消费模式。\", history=None)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edebb0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.27s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation import GenerationConfig\n",
    "\n",
    "tokenizer_origin = AutoTokenizer.from_pretrained(\"Qwen/Qwen-1_8B-Chat/\", trust_remote_code=True)\n",
    "model_origin = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen-1_8B-Chat/\",\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ").eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "529b4d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！有什么我可以帮助你的吗？\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response, history = model_origin.chat(tokenizer_origin, \"你好\", history=None)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50e15bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大学生是指不满18岁的学生，不包括已经毕业的在校生。如果你指的是大学毕业生，那么一般来说，他们在完成学业后通常在20多岁左右步入社会。当然，这也会因地区和专业而异，一些地方或专业的大学毕业年龄可能会提前到25岁或更早。\n"
     ]
    }
   ],
   "source": [
    "response, history = model_origin.chat(tokenizer_origin, \"大学生一般多少岁？\", history=None)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c9c610",
   "metadata": {},
   "outputs": [],
   "source": [
    "response, history = model_origin.chat(tokenizer_origin, \"总结下面这段文本的摘要，随着科技飞速发展，我们的生活方式发生巨大改变。手机、人工智能、物联网的出现，让日常生活便利而舒适。比如，我们可以通过智能手机随时随地地获取信息，控制家庭设备，同时感受着人工智能为我们带来的智能化之便。但是，科技进步带来的便利也会对生活形成某种影响，比如，冲击传统行业和职业，改变人们的生产和消费模式。\", history=None)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694de34f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
