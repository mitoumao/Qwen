{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e6981ab-2d9a-4280-923f-235a166855ba",
   "metadata": {},
   "source": [
    "# LoRA Fine-Tuning Qwen-Chat Large Language Model (Single GPU)\n",
    "\n",
    "Tongyi Qianwen is a large language model developed by Alibaba Cloud based on the Transformer architecture, trained on an extensive set of pre-training data. The pre-training data is diverse and covers a wide range, including a large amount of internet text, specialized books, code, etc. In addition, an AI assistant called Qwen-Chat has been created based on the pre-trained model using alignment mechanism.\n",
    "\n",
    "This notebook uses Qwen-1.8B-Chat as an example to introduce how to LoRA fine-tune the Qianwen model using Deepspeed.\n",
    "\n",
    "## Environment Requirements\n",
    "\n",
    "Please refer to **requirements.txt** to install the required dependencies.\n",
    "\n",
    "## Preparation\n",
    "\n",
    "### Download Qwen-1.8B-Chat\n",
    "\n",
    "First, download the model files. You can choose to download directly from ModelScope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f228a718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zjy/llm_journey/Qwen/recipes/finetune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zjy/llm_journey/Qwen/qwen/lib/python3.12/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/zjy/llm_journey/Qwen/recipes/finetune'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %pwd\n",
    "# %cd ../\n",
    "# %pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "248488f9-4a86-4f35-9d56-50f8e91a8f11",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model to directory: ./hub/Qwen/Qwen-1_8B-Chat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 10:27:55,084 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n",
      "Downloading [cache_autogptq_cuda_kernel_256.cu]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50.8k/50.8k [00:01<00:00, 30.7kB/s]\n",
      "Downloading [config.json]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 910/910 [00:01<00:00, 484B/s]\n",
      "Downloading [configuration.json]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77.0/77.0 [00:02<00:00, 31.4B/s]\n",
      "Downloading [configuration_qwen.py]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.29k/2.29k [00:01<00:00, 1.20kB/s]\n",
      "Downloading [cpp_kernels.py]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.88k/1.88k [00:01<00:00, 1.18kB/s]\n",
      "Downloading [generation_config.json]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 249/249 [00:01<00:00, 143B/s]\n",
      "Downloading [LICENSE]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.11k/7.11k [00:01<00:00, 5.06kB/s]\n",
      "Downloading [assets/logo.jpg]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80.8k/80.8k [00:01<00:00, 46.0kB/s]\n",
      "Downloading [model-00001-of-00002.safetensors]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.90G/1.90G [01:04<00:00, 31.9MB/s]\n",
      "Downloading [model-00002-of-00002.safetensors]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.52G/1.52G [00:57<00:00, 28.4MB/s]\n",
      "Downloading [model.safetensors.index.json]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14.4k/14.4k [00:01<00:00, 9.75kB/s]\n",
      "Downloading [modeling_qwen.py]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54.3k/54.3k [00:01<00:00, 30.9kB/s]\n",
      "Downloading [NOTICE]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15.0k/15.0k [00:01<00:00, 7.85kB/s]\n",
      "Downloading [qwen.tiktoken]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.44M/2.44M [00:02<00:00, 886kB/s]\n",
      "\n",
      "Downloading [qwen_generation_utils.py]:   0%|          | 0.00/14.3k [01:00<?, ?B/s]\n",
      "\n",
      "Downloading [qwen_generation_utils.py]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14.3k/14.3k [00:03<00:00, 4.70kB/s]\n",
      "Downloading [assets/qwen_tokenizer.png]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79.0k/79.0k [00:02<00:00, 38.3kB/s]\n",
      "Downloading [examples/react_prompt.md]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11.6k/11.6k [00:01<00:00, 7.84kB/s]\n",
      "Downloading [assets/react_showcase_001.png]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 302k/302k [00:02<00:00, 137kB/s]\n",
      "Downloading [assets/react_showcase_002.png]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 615k/615k [00:02<00:00, 236kB/s]\n",
      "Downloading [README.md]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25.9k/25.9k [00:02<00:00, 12.0kB/s]\n",
      "Downloading [tokenization_qwen.py]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.39k/9.39k [00:01<00:00, 6.70kB/s]\n",
      "Downloading [tokenizer_config.json]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:01<00:00, 129B/s]\n",
      "Downloading [assets/wechat.png]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 66.8k/66.8k [00:01<00:00, 38.4kB/s]\n"
     ]
    }
   ],
   "source": [
    "from modelscope.hub.snapshot_download import snapshot_download\n",
    "model_dir = snapshot_download('Qwen/Qwen-1_8B-Chat', cache_dir='.', revision='master')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b2a92b1-f08e-4413-9f92-8f23761e6e1f",
   "metadata": {},
   "source": [
    "### Download Example Training Data\n",
    "\n",
    "Download the data required for training; here, we provide a tiny dataset as an example. It is sampled from [Belle](https://github.com/LianjiaTech/BELLE).\n",
    "\n",
    "Disclaimer: the dataset can be only used for the research purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce195f08-fbb2-470e-b6c0-9a03457458c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-15 10:32:13--  https://atp-modelzoo-sh.oss-cn-shanghai.aliyuncs.com/release/tutorials/qwen_recipes/Belle_sampled_qwen.json\n",
      "Resolving atp-modelzoo-sh.oss-cn-shanghai.aliyuncs.com (atp-modelzoo-sh.oss-cn-shanghai.aliyuncs.com)... 47.101.88.43\n",
      "Connecting to atp-modelzoo-sh.oss-cn-shanghai.aliyuncs.com (atp-modelzoo-sh.oss-cn-shanghai.aliyuncs.com)|47.101.88.43|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 228189 (223K) [application/json]\n",
      "Saving to: â€˜Belle_sampled_qwen.jsonâ€™\n",
      "\n",
      "Belle_sampled_qwen. 100%[===================>] 222.84K   530KB/s    in 0.4s    \n",
      "\n",
      "2024-11-15 10:32:15 (530 KB/s) - â€˜Belle_sampled_qwen.jsonâ€™ saved [228189/228189]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://atp-modelzoo-sh.oss-cn-shanghai.aliyuncs.com/release/tutorials/qwen_recipes/Belle_sampled_qwen.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7226bed0-171b-4d45-a3f9-b3d81ec2bb9f",
   "metadata": {},
   "source": [
    "You can also refer to this format to prepare the dataset. Below is a simple example list with 1 sample:\n",
    "\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"id\": \"identity_0\",\n",
    "    \"conversations\": [\n",
    "      {\n",
    "        \"from\": \"user\",\n",
    "        \"value\": \"ä½ å¥½\"\n",
    "      },\n",
    "      {\n",
    "        \"from\": \"assistant\",\n",
    "        \"value\": \"æˆ‘æ˜¯ä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼Œæˆ‘å«é€šä¹‰åƒé—®ã€‚\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "You can also use multi-turn conversations as the training set. Here is a simple example:\n",
    "\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"id\": \"identity_0\",\n",
    "    \"conversations\": [\n",
    "      {\n",
    "        \"from\": \"user\",\n",
    "        \"value\": \"ä½ å¥½ï¼Œèƒ½å‘Šè¯‰æˆ‘é›ç‹—çš„æœ€ä½³æ—¶é—´å—ï¼Ÿ\"\n",
    "      },\n",
    "      {\n",
    "        \"from\": \"assistant\",\n",
    "        \"value\": \"å½“åœ°æœ€ä½³é›ç‹—æ—¶é—´å› åœ°åŸŸå·®å¼‚è€Œå¼‚ï¼Œè¯·é—®æ‚¨æ‰€åœ¨çš„åŸå¸‚æ˜¯å“ªé‡Œï¼Ÿ\"\n",
    "      },\n",
    "      {\n",
    "        \"from\": \"user\",\n",
    "        \"value\": \"æˆ‘åœ¨çº½çº¦å¸‚ã€‚\"\n",
    "      },\n",
    "      {\n",
    "        \"from\": \"assistant\",\n",
    "        \"value\": \"çº½çº¦å¸‚çš„é›ç‹—æœ€ä½³æ—¶é—´é€šå¸¸åœ¨æ—©æ™¨6ç‚¹è‡³8ç‚¹å’Œæ™šä¸Š8ç‚¹è‡³10ç‚¹ä¹‹é—´ï¼Œå› ä¸ºè¿™äº›æ—¶é—´æ®µæ°”æ¸©è¾ƒä½ï¼Œé›ç‹—æ›´åŠ èˆ’é€‚ã€‚ä½†å…·ä½“æ—¶é—´è¿˜éœ€æ ¹æ®æ°”å€™ã€æ°”æ¸©å’Œå­£èŠ‚å˜åŒ–è€Œå®šã€‚\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "## Fine-Tune the Model\n",
    "\n",
    "You can directly run the prepared training script to fine-tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ab0581e-be85-45e6-a5b7-af9c42ea697b",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-15 10:33:12,618] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to mps (auto detect)\n",
      "W1115 10:33:13.226000 34929 qwen/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "/Users/zjy/llm_journey/Qwen/qwen/lib/python3.12/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:06<00:00,  3.06s/it]\n",
      "trainable params: 53,673,984 || all params: 1,890,502,656 || trainable%: 2.8391\n",
      "Loading data...\n",
      "Formatting inputs...Skip in lazy mode\n",
      "/Users/zjy/llm_journey/Qwen/recipes/finetune/../../finetune.py:364: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n",
      "  0%|                                                    | 0/80 [00:00<?, ?it/s]/Users/zjy/llm_journey/Qwen/qwen/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "{'loss': 1.035, 'grad_norm': 5.1622772216796875, 'learning_rate': 1e-05, 'epoch': 0.06}\n",
      "{'loss': 1.0264, 'grad_norm': 4.078629016876221, 'learning_rate': 9.99604698613651e-06, 'epoch': 0.12}\n",
      "{'loss': 0.7152, 'grad_norm': 5.781802654266357, 'learning_rate': 9.98419419507348e-06, 'epoch': 0.18}\n",
      "{'loss': 0.825, 'grad_norm': 5.717673301696777, 'learning_rate': 9.964460368509868e-06, 'epoch': 0.24}\n",
      "{'loss': 0.9382, 'grad_norm': 5.715182781219482, 'learning_rate': 9.936876709681668e-06, 'epoch': 0.3}\n",
      "{'loss': 0.7334, 'grad_norm': 4.8090643882751465, 'learning_rate': 9.901486834023182e-06, 'epoch': 0.36}\n",
      "{'loss': 0.968, 'grad_norm': 6.176565170288086, 'learning_rate': 9.85834670020205e-06, 'epoch': 0.42}\n",
      "{'loss': 0.9656, 'grad_norm': 4.485891342163086, 'learning_rate': 9.807524521637103e-06, 'epoch': 0.48}\n",
      "{'loss': 0.9317, 'grad_norm': 6.785899639129639, 'learning_rate': 9.749100658638914e-06, 'epoch': 0.54}\n",
      "{'loss': 0.6533, 'grad_norm': 4.353843688964844, 'learning_rate': 9.68316749134364e-06, 'epoch': 0.6}\n",
      "{'loss': 0.9021, 'grad_norm': 5.597474098205566, 'learning_rate': 9.609829273641034e-06, 'epoch': 0.66}\n",
      "{'loss': 0.6869, 'grad_norm': 6.517662525177002, 'learning_rate': 9.529201968327618e-06, 'epoch': 0.72}\n",
      "{'loss': 0.8264, 'grad_norm': 4.081151962280273, 'learning_rate': 9.44141306374566e-06, 'epoch': 0.78}\n",
      "{'loss': 0.8148, 'grad_norm': 5.576452732086182, 'learning_rate': 9.346601372197914e-06, 'epoch': 0.84}\n",
      "{'loss': 0.8326, 'grad_norm': 6.580228328704834, 'learning_rate': 9.244916810456822e-06, 'epoch': 0.9}\n",
      "{'loss': 0.8467, 'grad_norm': 4.5379838943481445, 'learning_rate': 9.136520162715288e-06, 'epoch': 0.96}\n",
      "{'loss': 0.9218, 'grad_norm': 6.246469974517822, 'learning_rate': 9.021582826353825e-06, 'epoch': 1.02}\n",
      "{'loss': 0.8494, 'grad_norm': 4.972136974334717, 'learning_rate': 8.900286540926062e-06, 'epoch': 1.08}\n",
      "{'loss': 0.7554, 'grad_norm': 4.3841986656188965, 'learning_rate': 8.772823100791152e-06, 'epoch': 1.14}\n",
      "{'loss': 0.8978, 'grad_norm': 4.059811115264893, 'learning_rate': 8.639394051847472e-06, 'epoch': 1.2}\n",
      "{'loss': 0.8493, 'grad_norm': 5.9042840003967285, 'learning_rate': 8.500210372847128e-06, 'epoch': 1.26}\n",
      "{'loss': 0.7753, 'grad_norm': 4.504127502441406, 'learning_rate': 8.355492141795185e-06, 'epoch': 1.32}\n",
      "{'loss': 0.7823, 'grad_norm': 4.070717811584473, 'learning_rate': 8.2054681879611e-06, 'epoch': 1.38}\n",
      "{'loss': 0.9039, 'grad_norm': 4.480990886688232, 'learning_rate': 8.050375730052623e-06, 'epoch': 1.44}\n",
      "{'loss': 0.8019, 'grad_norm': 4.179333209991455, 'learning_rate': 7.890460001124242e-06, 'epoch': 1.5}\n",
      "{'loss': 0.964, 'grad_norm': 4.743725299835205, 'learning_rate': 7.725973860813338e-06, 'epoch': 1.56}\n",
      "{'loss': 0.9067, 'grad_norm': 5.047615051269531, 'learning_rate': 7.5571773955171124e-06, 'epoch': 1.62}\n",
      "{'loss': 0.8181, 'grad_norm': 5.650163173675537, 'learning_rate': 7.3843375071425315e-06, 'epoch': 1.68}\n",
      "{'loss': 0.9456, 'grad_norm': 5.578127384185791, 'learning_rate': 7.2077274910795605e-06, 'epoch': 1.74}\n",
      "{'loss': 0.6889, 'grad_norm': 3.474552631378174, 'learning_rate': 7.02762660406497e-06, 'epoch': 1.8}\n",
      "{'loss': 0.6942, 'grad_norm': 4.596667766571045, 'learning_rate': 6.844319622620039e-06, 'epoch': 1.86}\n",
      "{'loss': 0.4909, 'grad_norm': 3.7791898250579834, 'learning_rate': 6.65809639276034e-06, 'epoch': 1.92}\n",
      "{'loss': 0.734, 'grad_norm': 3.2934184074401855, 'learning_rate': 6.469251371689606e-06, 'epoch': 1.98}\n",
      "{'loss': 0.8492, 'grad_norm': 5.371791362762451, 'learning_rate': 6.278083162202374e-06, 'epoch': 2.04}\n",
      "{'loss': 0.7505, 'grad_norm': 4.758277893066406, 'learning_rate': 6.084894040531591e-06, 'epoch': 2.1}\n",
      "{'loss': 0.7726, 'grad_norm': 4.501697063446045, 'learning_rate': 5.8899894783877536e-06, 'epoch': 2.16}\n",
      "{'loss': 0.6748, 'grad_norm': 4.079547882080078, 'learning_rate': 5.693677659945343e-06, 'epoch': 2.22}\n",
      "{'loss': 0.6691, 'grad_norm': 4.856786727905273, 'learning_rate': 5.496268994540309e-06, 'epoch': 2.28}\n",
      "{'loss': 0.6963, 'grad_norm': 3.4810547828674316, 'learning_rate': 5.2980756258491e-06, 'epoch': 2.34}\n",
      "{'loss': 0.8492, 'grad_norm': 4.2446441650390625, 'learning_rate': 5.099410938325351e-06, 'epoch': 2.4}\n",
      "{'loss': 0.882, 'grad_norm': 4.098357200622559, 'learning_rate': 4.900589061674649e-06, 'epoch': 2.46}\n",
      "{'loss': 0.8962, 'grad_norm': 3.8160812854766846, 'learning_rate': 4.701924374150901e-06, 'epoch': 2.52}\n",
      "{'loss': 0.6679, 'grad_norm': 4.80321741104126, 'learning_rate': 4.5037310054596936e-06, 'epoch': 2.58}\n",
      "{'loss': 0.905, 'grad_norm': 4.543826103210449, 'learning_rate': 4.30632234005466e-06, 'epoch': 2.64}\n",
      "{'loss': 0.6998, 'grad_norm': 4.6856255531311035, 'learning_rate': 4.11001052161225e-06, 'epoch': 2.7}\n",
      "{'loss': 0.6735, 'grad_norm': 3.9663684368133545, 'learning_rate': 3.91510595946841e-06, 'epoch': 2.76}\n",
      "{'loss': 0.7922, 'grad_norm': 4.065088272094727, 'learning_rate': 3.721916837797627e-06, 'epoch': 2.82}\n",
      "{'loss': 0.8319, 'grad_norm': 4.772068023681641, 'learning_rate': 3.5307486283103966e-06, 'epoch': 2.88}\n",
      "{'loss': 0.8971, 'grad_norm': 4.337107181549072, 'learning_rate': 3.3419036072396614e-06, 'epoch': 2.94}\n",
      "{'loss': 0.8454, 'grad_norm': 4.671784400939941, 'learning_rate': 3.1556803773799616e-06, 'epoch': 3.0}\n",
      "{'loss': 0.754, 'grad_norm': 4.853881359100342, 'learning_rate': 2.972373395935031e-06, 'epoch': 3.06}\n",
      "{'loss': 0.8493, 'grad_norm': 3.6191303730010986, 'learning_rate': 2.792272508920443e-06, 'epoch': 3.12}\n",
      "{'loss': 0.6945, 'grad_norm': 4.216838359832764, 'learning_rate': 2.615662492857471e-06, 'epoch': 3.18}\n",
      "{'loss': 0.6956, 'grad_norm': 4.112166881561279, 'learning_rate': 2.4428226044828896e-06, 'epoch': 3.24}\n",
      "{'loss': 0.8322, 'grad_norm': 4.0934600830078125, 'learning_rate': 2.2740261391866634e-06, 'epoch': 3.3}\n",
      "{'loss': 0.7941, 'grad_norm': 4.693774223327637, 'learning_rate': 2.109539998875757e-06, 'epoch': 3.36}\n",
      "{'loss': 0.838, 'grad_norm': 4.202505111694336, 'learning_rate': 1.949624269947378e-06, 'epoch': 3.42}\n",
      "{'loss': 0.5498, 'grad_norm': 3.8740243911743164, 'learning_rate': 1.794531812038901e-06, 'epoch': 3.48}\n",
      "{'loss': 0.6835, 'grad_norm': 4.039596080780029, 'learning_rate': 1.6445078582048158e-06, 'epoch': 3.54}\n",
      "{'loss': 0.7872, 'grad_norm': 3.8412694931030273, 'learning_rate': 1.499789627152874e-06, 'epoch': 3.6}\n",
      "{'loss': 0.7616, 'grad_norm': 4.56403112411499, 'learning_rate': 1.3606059481525296e-06, 'epoch': 3.66}\n",
      "{'loss': 0.7432, 'grad_norm': 4.36243200302124, 'learning_rate': 1.227176899208849e-06, 'epoch': 3.72}\n",
      "{'loss': 0.8363, 'grad_norm': 3.5047082901000977, 'learning_rate': 1.09971345907394e-06, 'epoch': 3.78}\n",
      "{'loss': 0.731, 'grad_norm': 4.156765460968018, 'learning_rate': 9.784171736461762e-07, 'epoch': 3.84}\n",
      "{'loss': 0.6459, 'grad_norm': 3.7369439601898193, 'learning_rate': 8.634798372847148e-07, 'epoch': 3.9}\n",
      "{'loss': 0.8721, 'grad_norm': 5.273192882537842, 'learning_rate': 7.550831895431799e-07, 'epoch': 3.96}\n",
      "{'loss': 1.0929, 'grad_norm': 4.706140995025635, 'learning_rate': 6.533986278020876e-07, 'epoch': 4.01}\n",
      "{'loss': 0.8627, 'grad_norm': 4.617386817932129, 'learning_rate': 5.585869362543416e-07, 'epoch': 4.07}\n",
      "{'loss': 0.7376, 'grad_norm': 4.293893814086914, 'learning_rate': 4.707980316723837e-07, 'epoch': 4.13}\n",
      "{'loss': 0.8108, 'grad_norm': 4.994075775146484, 'learning_rate': 3.9017072635896716e-07, 'epoch': 4.19}\n",
      "{'loss': 0.8539, 'grad_norm': 3.4912614822387695, 'learning_rate': 3.168325086563612e-07, 'epoch': 4.25}\n",
      "{'loss': 0.7904, 'grad_norm': 4.384023666381836, 'learning_rate': 2.5089934136108665e-07, 'epoch': 4.31}\n",
      "{'loss': 0.7134, 'grad_norm': 3.9337055683135986, 'learning_rate': 1.9247547836289792e-07, 'epoch': 4.37}\n",
      "{'loss': 0.9315, 'grad_norm': 4.131223201751709, 'learning_rate': 1.4165329979794972e-07, 'epoch': 4.43}\n",
      "{'loss': 0.8102, 'grad_norm': 3.969045400619507, 'learning_rate': 9.851316597681959e-08, 'epoch': 4.49}\n",
      "{'loss': 0.7966, 'grad_norm': 4.422358512878418, 'learning_rate': 6.31232903183332e-08, 'epoch': 4.55}\n",
      "{'loss': 0.6032, 'grad_norm': 4.000452518463135, 'learning_rate': 3.553963149013295e-08, 'epoch': 4.61}\n",
      "{'loss': 0.6571, 'grad_norm': 4.284400939941406, 'learning_rate': 1.580580492652084e-08, 'epoch': 4.67}\n",
      "{'loss': 0.7421, 'grad_norm': 4.000175476074219, 'learning_rate': 3.953013863490784e-09, 'epoch': 4.73}\n",
      "{'loss': 0.8212, 'grad_norm': 4.563438415527344, 'learning_rate': 0.0, 'epoch': 4.79}\n",
      "{'train_runtime': 5887.729, 'train_samples_per_second': 0.227, 'train_steps_per_second': 0.014, 'train_loss': 0.8015662714838981, 'epoch': 4.79}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80/80 [1:38:07<00:00, 73.60s/it]\n"
     ]
    }
   ],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0\n",
    "!python ../../finetune.py \\\n",
    "    --model_name_or_path \"Qwen/Qwen-1_8B-Chat/\"\\\n",
    "    --data_path  \"Belle_sampled_qwen.json\"\\\n",
    "    --bf16 \\\n",
    "    --output_dir \"output_qwen\" \\\n",
    "    --num_train_epochs 5 \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --gradient_accumulation_steps 16 \\\n",
    "    --evaluation_strategy \"no\" \\\n",
    "    --save_strategy \"steps\" \\\n",
    "    --save_steps 1000 \\\n",
    "    --save_total_limit 10 \\\n",
    "    --learning_rate 1e-5 \\\n",
    "    --weight_decay 0.1 \\\n",
    "    --adam_beta2 0.95 \\\n",
    "    --warmup_ratio 0.01 \\\n",
    "    --lr_scheduler_type \"cosine\" \\\n",
    "    --logging_steps 1 \\\n",
    "    --report_to \"none\" \\\n",
    "    --model_max_length 512 \\\n",
    "    --gradient_checkpointing \\\n",
    "    --lazy_preprocess \\\n",
    "    --use_lora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6f28aa-1772-48ce-aa15-8cf29e7d67b5",
   "metadata": {},
   "source": [
    "## Merge Weights\n",
    "\n",
    "The training of both LoRA and Q-LoRA only saves the adapter parameters. You can load the fine-tuned model and merge weights as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fd5ef2a-34f9-4909-bebe-7b3b086fd16a",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-15 12:12:15,849] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to mps (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1115 12:12:16.075000 33825 torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-1_8B-Chat/\", torch_dtype=torch.float16, device_map=\"auto\", trust_remote_code=True)\n",
    "model = PeftModel.from_pretrained(model, \"output_qwen/\")\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"output_qwen_merged\", max_shard_size=\"2048MB\", safe_serialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3f5b9f-63a1-4599-8d9b-a8d8f764838f",
   "metadata": {},
   "source": [
    "The tokenizer files are not saved in the new directory in this step. You can copy the tokenizer files or use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10fa5ea3-dd55-4901-86af-c045d4c56533",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('output_qwen_merged/tokenizer_config.json',\n",
       " 'output_qwen_merged/special_tokens_map.json',\n",
       " 'output_qwen_merged/qwen.tiktoken',\n",
       " 'output_qwen_merged/added_tokens.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Qwen/Qwen-1_8B-Chat/\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer.save_pretrained(\"output_qwen_merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804b84d8",
   "metadata": {},
   "source": [
    "## Test the Model\n",
    "\n",
    "### added the modeling_qwen.py, qwen_generation_utils.py, qwen_generation_utils.py from original Qwen/Qwen-1_8B-Chat\n",
    "\n",
    "After merging the weights, we can test the model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading [cache_autogptq_cuda_kernel_256.cu]:   0%|          | 0.00/50.8k [1:50:45<?, ?B/s]\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.92s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the disk.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘èƒ½å¸®åŠ©ä½ çš„å—ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation import GenerationConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"output_qwen_merged\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"output_qwen_merged\",\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ").eval()\n",
    "\n",
    "response, history = model.chat(tokenizer, \"ä½ å¥½\", history=None)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ede03cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, \"ä½ å¥½\", history=None)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "099d4226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä¸­å›½çš„é¦–éƒ½æ˜¯åŒ—äº¬ã€‚\n"
     ]
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, \"è¯·é—®ä¸­å›½çš„é¦–éƒ½æ˜¯å“ªä¸ªåŸå¸‚ï¼Ÿ\", history=None)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ee65f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç°åœ¨çš„æ³•å®šç»“å©šå¹´é¾„æ˜¯ç”·æ»¡20å‘¨å²ï¼Œå¥³æ»¡18å‘¨å²ã€‚\n"
     ]
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, \"å¤§å­¦ç”Ÿä¸€èˆ¬å¤šå°‘å²ï¼Ÿ\", history=None)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c96360b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "éšç€ç§‘æŠ€çš„å‘å±•ï¼Œäººä»¬çš„ç”Ÿæ´»æ–¹å¼å‘ç”Ÿäº†ç¿»å¤©è¦†åœ°çš„å˜åŒ–ã€‚æ‰‹æœºã€äººå·¥æ™ºèƒ½ç­‰æŠ€æœ¯ä½¿å¾—ç”Ÿæ´»æ›´ä¸ºä¾¿æ·å’Œèˆ’é€‚ã€‚ç„¶è€Œï¼Œè¿™ä¹Ÿå¯èƒ½å¸¦æ¥ä¸€äº›è´Ÿé¢å½±å“ï¼Œå¦‚ä¼ ç»Ÿè¡Œä¸šå’ŒèŒä¸šçš„å†²å‡»ä»¥åŠç”Ÿäº§æ¶ˆè´¹æ¨¡å¼çš„æ”¹å˜ã€‚\n"
     ]
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, \"æ€»ç»“ä¸‹é¢è¿™æ®µæ–‡æœ¬çš„æ‘˜è¦ï¼Œéšç€ç§‘æŠ€é£é€Ÿå‘å±•ï¼Œæˆ‘ä»¬çš„ç”Ÿæ´»æ–¹å¼å‘ç”Ÿå·¨å¤§æ”¹å˜ã€‚æ‰‹æœºã€äººå·¥æ™ºèƒ½ã€ç‰©è”ç½‘çš„å‡ºç°ï¼Œè®©æ—¥å¸¸ç”Ÿæ´»ä¾¿åˆ©è€Œèˆ’é€‚ã€‚æ¯”å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ™ºèƒ½æ‰‹æœºéšæ—¶éšåœ°åœ°è·å–ä¿¡æ¯ï¼Œæ§åˆ¶å®¶åº­è®¾å¤‡ï¼ŒåŒæ—¶æ„Ÿå—ç€äººå·¥æ™ºèƒ½ä¸ºæˆ‘ä»¬å¸¦æ¥çš„æ™ºèƒ½åŒ–ä¹‹ä¾¿ã€‚ä½†æ˜¯ï¼Œç§‘æŠ€è¿›æ­¥å¸¦æ¥çš„ä¾¿åˆ©ä¹Ÿä¼šå¯¹ç”Ÿæ´»å½¢æˆæŸç§å½±å“ï¼Œæ¯”å¦‚ï¼Œå†²å‡»ä¼ ç»Ÿè¡Œä¸šå’ŒèŒä¸šï¼Œæ”¹å˜äººä»¬çš„ç”Ÿäº§å’Œæ¶ˆè´¹æ¨¡å¼ã€‚\", history=None)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e20c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "response, history = model.chat(tokenizer, \"æ€»ç»“ä¸‹é¢è¿™æ®µæ–‡æœ¬çš„æ‘˜è¦ï¼Œéšç€ç§‘æŠ€çš„é£é€Ÿå‘å±•ï¼Œæˆ‘ä»¬çš„ç”Ÿæ´»æ–¹å¼ä¹Ÿåœ¨æ‚„ç„¶æ”¹å˜ã€‚æ™ºèƒ½æ‰‹æœºã€äººå·¥æ™ºèƒ½ã€ç‰©è”ç½‘ç­‰ç§‘æŠ€äº§å“çš„å‡ºç°ï¼Œä¸ºæˆ‘ä»¬çš„æ—¥å¸¸ç”Ÿæ´»å¸¦æ¥äº†æ›´å¤šä¾¿åˆ©å’Œèˆ’é€‚ã€‚æ¯”å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ™ºèƒ½æ‰‹æœºéšæ—¶éšåœ°åœ°è·å–ä¿¡æ¯ï¼Œæ§åˆ¶å®¶åº­è®¾å¤‡ï¼ŒåŒæ—¶æ„Ÿå—ç€äººå·¥æ™ºèƒ½ä¸ºæˆ‘ä»¬å¸¦æ¥çš„æ™ºèƒ½åŒ–ä¹‹ä¾¿ã€‚ä½†æ˜¯ï¼Œç§‘æŠ€è¿›æ­¥å¸¦æ¥çš„ä¾¿åˆ©ä¹Ÿä¼šå¯¹ç”Ÿæ´»å½¢æˆæŸç§å½±å“ï¼Œæ¯”å¦‚ï¼Œå†²å‡»ä¼ ç»Ÿè¡Œä¸šå’ŒèŒä¸šï¼Œæ”¹å˜äººä»¬çš„ç”Ÿäº§å’Œæ¶ˆè´¹æ¨¡å¼ã€‚\", history=None)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edebb0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.27s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation import GenerationConfig\n",
    "\n",
    "tokenizer_origin = AutoTokenizer.from_pretrained(\"Qwen/Qwen-1_8B-Chat/\", trust_remote_code=True)\n",
    "model_origin = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen-1_8B-Chat/\",\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ").eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "529b4d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response, history = model_origin.chat(tokenizer_origin, \"ä½ å¥½\", history=None)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50e15bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¤§å­¦ç”Ÿæ˜¯æŒ‡ä¸æ»¡18å²çš„å­¦ç”Ÿï¼Œä¸åŒ…æ‹¬å·²ç»æ¯•ä¸šçš„åœ¨æ ¡ç”Ÿã€‚å¦‚æœä½ æŒ‡çš„æ˜¯å¤§å­¦æ¯•ä¸šç”Ÿï¼Œé‚£ä¹ˆä¸€èˆ¬æ¥è¯´ï¼Œä»–ä»¬åœ¨å®Œæˆå­¦ä¸šåé€šå¸¸åœ¨20å¤šå²å·¦å³æ­¥å…¥ç¤¾ä¼šã€‚å½“ç„¶ï¼Œè¿™ä¹Ÿä¼šå› åœ°åŒºå’Œä¸“ä¸šè€Œå¼‚ï¼Œä¸€äº›åœ°æ–¹æˆ–ä¸“ä¸šçš„å¤§å­¦æ¯•ä¸šå¹´é¾„å¯èƒ½ä¼šæå‰åˆ°25å²æˆ–æ›´æ—©ã€‚\n"
     ]
    }
   ],
   "source": [
    "response, history = model_origin.chat(tokenizer_origin, \"å¤§å­¦ç”Ÿä¸€èˆ¬å¤šå°‘å²ï¼Ÿ\", history=None)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c9c610",
   "metadata": {},
   "outputs": [],
   "source": [
    "response, history = model_origin.chat(tokenizer_origin, \"æ€»ç»“ä¸‹é¢è¿™æ®µæ–‡æœ¬çš„æ‘˜è¦ï¼Œéšç€ç§‘æŠ€é£é€Ÿå‘å±•ï¼Œæˆ‘ä»¬çš„ç”Ÿæ´»æ–¹å¼å‘ç”Ÿå·¨å¤§æ”¹å˜ã€‚æ‰‹æœºã€äººå·¥æ™ºèƒ½ã€ç‰©è”ç½‘çš„å‡ºç°ï¼Œè®©æ—¥å¸¸ç”Ÿæ´»ä¾¿åˆ©è€Œèˆ’é€‚ã€‚æ¯”å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ™ºèƒ½æ‰‹æœºéšæ—¶éšåœ°åœ°è·å–ä¿¡æ¯ï¼Œæ§åˆ¶å®¶åº­è®¾å¤‡ï¼ŒåŒæ—¶æ„Ÿå—ç€äººå·¥æ™ºèƒ½ä¸ºæˆ‘ä»¬å¸¦æ¥çš„æ™ºèƒ½åŒ–ä¹‹ä¾¿ã€‚ä½†æ˜¯ï¼Œç§‘æŠ€è¿›æ­¥å¸¦æ¥çš„ä¾¿åˆ©ä¹Ÿä¼šå¯¹ç”Ÿæ´»å½¢æˆæŸç§å½±å“ï¼Œæ¯”å¦‚ï¼Œå†²å‡»ä¼ ç»Ÿè¡Œä¸šå’ŒèŒä¸šï¼Œæ”¹å˜äººä»¬çš„ç”Ÿäº§å’Œæ¶ˆè´¹æ¨¡å¼ã€‚\", history=None)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694de34f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
